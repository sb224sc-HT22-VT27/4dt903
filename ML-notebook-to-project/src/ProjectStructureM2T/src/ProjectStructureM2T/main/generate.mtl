[module generate('http://www.g7.org/projectStructureMM')]

[template public generateProject(p : ProjectStructure)]
[comment @main/]
[for (e : FileSystemElement | p.filesystemelement)]
  [generateElement(e, p.name + '/')/]
[/for]
[comment Generate Dockerfile in M2T/]
[generateDockerfile(p)/]
[comment Generate .dockerignore in M2T/]
[generateDockerignore(p)/]
[comment Generate server.py in M2T/]
[generateServerPy(p)/]
[/template]

[template private generateElement(e : FileSystemElement, path : String)]
[if (e.oclIsTypeOf(Directory))]
  [file (path + e.name + '/.keep', false, 'UTF-8')]
  [/file]
  [for (sub : FileSystemElement | e.oclAsType(Directory).filesystemelement)]
    [generateElement(sub, path + e.name + '/')/]
  [/for]
[/if]
[if (e.oclIsTypeOf(File))]
  [file (path + e.name, false, 'UTF-8')]
[for (line : String | e.oclAsType(File).content)]
[line/]
[/for]
  [/file]
[/if]
[/template]

[template private generateDockerfile(p : ProjectStructure)]
[file (p.name + '/Dockerfile', false, 'UTF-8')]
FROM python:3.13-slim

WORKDIR /app

# Copy requirements first (for better caching)
COPY requirements.txt .

# Set up virtual environment and install dependencies
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install dependencies
RUN pip install --upgrade pip && \
    pip install -r requirements.txt

# Copy project files
COPY src/ ./src/
COPY data/ ./data/
COPY models/ ./models/
COPY outputs/ ./outputs/

# Expose the port for prediction service
EXPOSE 8080

# Run the server
CMD ["python", "src/server.py"]
[/file]
[/template]

[template private generateDockerignore(p : ProjectStructure)]
[file (p.name + '/.dockerignore', false, 'UTF-8')]
__pycache__
*.pyc
.git
.env
[/file]
[/template]

[template private generateServerPy(p : ProjectStructure)]
[file (p.name + '/src/server.py', false, 'UTF-8')]
"""
Prediction service - Flask based API for model prediction
Accessible at localhost:8080/predict
"""

from flask import Flask, request, jsonify
import joblib
import numpy as np
import os

app = Flask(__name__)

# Load models and scalers at start
MODEL_PATH = os.path.join(os.path.dirname(__file__), "..", "models")
model = None
scaler = None
feature_names = None
is_keras_model = False

def load_models():
    """Load the trained model and associated artifacts."""
    global model, scaler, feature_names, is_keras_model
    try:
        model_files = [f for f in os.listdir(MODEL_PATH) if f.endswith(".pkl") or f.endswith(".keras") or f.endswith(".h5")]
        # Prioritize Keras and h5 models over pkl
        keras_models = [f for f in model_files if f.endswith(".keras") or f.endswith(".h5")]
        pkl_files = [f for f in model_files if f.endswith(".pkl")]
        
        for f in keras_models:
            if model is None:
                filepath = os.path.join(MODEL_PATH, f)
                try:
                    from tensorflow import keras
                    model = keras.models.load_model(filepath)
                    is_keras_model = True
                    print(f"Loaded Keras model from {f}")
                except ImportError:
                    print("TensorFlow not available for loading .keras/.h5 model")
        
        for f in pkl_files:
            filepath = os.path.join(MODEL_PATH, f)
            if model is None and "model" in f.lower() and "scaler" not in f.lower():
                model = joblib.load(filepath)
                print(f"Loaded model from {f}")
            elif "scaler" in f.lower():
                scaler = joblib.load(filepath)
                print(f"Loaded scaler from {f}")
            elif "feature_names" in f.lower():
                feature_names = joblib.load(filepath)
                print(f"Loaded feature names from {f}")
        
        if model is not None:
            print("Model loaded successfully")
        else:
            print("Warning: No model found in models directory")
    
    except Exception as e:
        print(f"Error loading model: {e}")

@app.route("/health", methods=["GET"])
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "model_loaded": model is not None
    })

@app.route("/predict", methods=["POST"])
def predict():
    """
    Prediction endpoint
    Expects JSON input with features for prediction.
    Returns JSON with prediction results
    
    Example input:
    {
        "features": [[1.0, 2.0, 3.0, 4.0]]
    }
    """
    try:
        if model is None:
            return jsonify({"error": "Model not loaded"}), 500
        
        data = request.get_json()
        if data is None:
            return jsonify({"error": "No JSON data provided"}), 400
        
        features = data.get("features")
        if features is None:
            return jsonify({"error": "No features provided"}), 400
        
        # Convert to numpy array
        features_array = np.array(features)
        
        # Apply scaling if scaler is available
        if scaler is not None:
            features_array = scaler.transform(features_array)
        
        # Make prediction
        if is_keras_model:
            predictions = model.predict(features_array)
            predictions = predictions.tolist()
        else:
            predictions = model.predict(features_array)
            predictions = predictions.tolist()
        
        # Get prediction probabilities if available
        try:
            if hasattr(model, 'predict_proba'):
                probabilities = model.predict_proba(features_array)
                return jsonify({
                    "predictions": predictions,
                    "probabilities": probabilities.tolist()
                })
        except:
            pass
        
        return jsonify({"predictions": predictions})
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    load_models()
    app.run(host="0.0.0.0", port=8080, debug=False)
[/file]
[/template]

